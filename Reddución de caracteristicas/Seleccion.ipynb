{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from string import ascii_uppercase\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = np.loadtxt('../Frogs_MFCCs.data', delimiter = ',')\n",
    "#Bufonidae - Rhinella - Rhinellagranulosa\n",
    "#Dendrobatidae - Ameerega - Ameeregatrivittata\n",
    "#Hylidae - 1.Dendropsophus, 2.Hypsiboas, 3.Osteocephalus, 4.Scinax - 1.HylaMinuta, 2.HypsiboasCinerascens, 2.HypsiboasCordobae, 3.OsteocephalusOophagus, 4.ScinaxRuber\n",
    "#Leptodactylidae - 1.Adenomera, 2.Leptodactylus - 1.AdenomeraAndre, 1.AdenomeraHylaedactylus, 2.LeptodactylusFuscus\n",
    "\n",
    "#Muestras\n",
    "X = db[:, 0:22]\n",
    "\n",
    "#Familias\n",
    "Y = db[:, 22:25]\n",
    "\n",
    "#Grupos\n",
    "G = db[:, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(y_est, y_real):\n",
    "    err = 0\n",
    "    for y_e, y_r in zip(y_est, y_real):\n",
    "\n",
    "        if y_e != y_r:\n",
    "            err += 1\n",
    "\n",
    "    return err/np.size(y_est)\n",
    "\n",
    "def select_features(modelo, n_features, fwd, fltg):\n",
    "    sfs = SFS(modelo, \n",
    "           k_features=n_features,\n",
    "           forward=fwd,\n",
    "           floating=fltg,\n",
    "           verbose=1,\n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "    \n",
    "    return sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleccion(features):\n",
    "    \n",
    "    fwd = True\n",
    "    fltg = False\n",
    "        \n",
    "    #Para calcular el costo computacional\n",
    "    initial_time = time.time()\n",
    "    \n",
    "    sensitivity_array = np.zeros([4,10])\n",
    "    efficiency_array = np.zeros([4])\n",
    "    error = np.zeros(4)\n",
    "    precision_array = np.zeros([4,10])\n",
    "    fscore_array = np.zeros([4,10])\n",
    "    TN_array = np.zeros([4])\n",
    "    FP_array = np.zeros([4])\n",
    "    FN_array = np.zeros([4])\n",
    "    TP_array = np.zeros([4])\n",
    "    \n",
    "    #Implemetamos la metodología de validación \n",
    "    SupportVC = SVC(decision_function_shape='ovo', kernel='rbf', C=10, gamma=1)\n",
    "    \n",
    "    for j in range(4):\n",
    "        Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y[:,2], test_size=0.25) # Modificar metodología de validación\n",
    "        scaler = MinMaxScaler() #Escala entre 0 y 1\n",
    "        Xtrain = scaler.fit_transform(Xtrain)\n",
    "        Xtest = scaler.transform(Xtest)\n",
    "\n",
    "        sfs = select_features(SupportVC, features, fwd, fltg)\n",
    "        sfs.fit(Xtrain, Ytrain)\n",
    "\n",
    "        X_train_sfs = sfs.transform(Xtrain)\n",
    "        X_test_sfs = sfs.transform(Xtest)\n",
    "        X_features = sfs.transform(X)\n",
    "\n",
    "        SupportVC.fit(X_train_sfs, Ytrain)\n",
    "        Yest = SupportVC.predict(X_test_sfs)\n",
    "        \n",
    "        #code for calculating recall \n",
    "        sensitivity = recall_score(Ytest, Yest, average=None)\n",
    "        sensitivity_array[j] = sensitivity\n",
    "\n",
    "        #code for calculating accuracy \n",
    "        efficiency = accuracy_score(Ytest, Yest, normalize=True)\n",
    "        efficiency_array[j] = efficiency\n",
    "\n",
    "        #code for calculating precision \n",
    "        precision = precision_score(Ytest, Yest, average=None)\n",
    "        precision_array[j] = precision\n",
    "\n",
    "        #code for calculating f1 score \n",
    "        fscore = f1_score(Ytest, Yest, average=None)\n",
    "        fscore_array[j] = fscore\n",
    "\n",
    "        error[j] = classification_error(Yest, Ytest)\n",
    "        \n",
    "    print(sfs.k_feature_idx_)    \n",
    "    return X_features, X_train_sfs, X_test_sfs, str(np.mean(efficiency_array)), str(np.std(efficiency_array)), str(np.mean(sensitivity_array)), str(np.std(sensitivity_array)), str(np.mean(precision_array)), str(np.std(precision_array)),  str(np.mean(fscore_array)), str(np.std(fscore_array)), str(np.mean(error)), str(np.std(error)), str(time.time()-initial_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "STOPPING EARLY DUE TO KEYBOARD INTERRUPT..."
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "randn = np.random.randn\n",
    "df_types = pd.DataFrame({'# de características seleccionadas' : pd.Series([22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3])})\n",
    "df_types[\"Eficiencia\"] = \"\"\n",
    "df_types[\"Int_Eficiencia\"] = \"\"\n",
    "df_types[\"Sensibilidad\"] = \"\"\n",
    "df_types[\"Int_Sensibilidad\"] = \"\"\n",
    "df_types[\"Precision\"] = \"\"\n",
    "df_types[\"Int_Precision\"] = \"\"\n",
    "df_types[\"F-Score\"] = \"\"\n",
    "df_types[\"Int_F-Score\"] = \"\"\n",
    "df_types[\"Error_Prueba\"] = \"\"\n",
    "df_types[\"Int_error\"] = \"\"\n",
    "df_types[\"Tiempo de ejecución\"] = \"\"\n",
    "df_types.set_index(['# de características seleccionadas'], inplace=True)\n",
    "\n",
    "for k in df_types.index:\n",
    "    XN, XtrainN, XtestN, efficiency, efficiency_interval, sensitivity, sensitivity_interval, precision, precision_interval, f, f_interval, error, std_error, tiempo = seleccion(k)\n",
    "    print(k)\n",
    "    df_types[\"Eficiencia\"][k] = efficiency\n",
    "    df_types[\"Int_Eficiencia\"][k] = efficiency_interval\n",
    "    df_types[\"Sensibilidad\"][k] = sensitivity\n",
    "    df_types[\"Int_Sensibilidad\"][k] = sensitivity_interval\n",
    "    df_types[\"Precision\"][k] = precision\n",
    "    df_types[\"Int_Precision\"][k] = precision_interval\n",
    "    df_types[\"F-Score\"][k] = f\n",
    "    df_types[\"Int_F-Score\"][k] = f_interval\n",
    "    df_types[\"Error_Prueba\"][k] = error\n",
    "    df_types[\"Int_error\"][k] = std_error\n",
    "    df_types[\"Tiempo de ejecución\"][k] = tiempo\n",
    "    \n",
    "qgrid_widget = qgrid.show_grid(df_types, show_toolbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgrid_widget.get_changed_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features, XtrainN, XtestN, Acc, IntAcc, Sen, IntSen, Pre, IntPre, f, IntF, error, stdError, tiempo = seleccion(21)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = X[:, [0, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 17, 18, 20, 21]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_model_CV(graficar = False):\n",
    "    initial_time = time.time()\n",
    "    sensitivity_array = np.zeros([4,10])\n",
    "    efficiency_array = np.zeros([4])\n",
    "    error = np.zeros(4)\n",
    "    precision_array = np.zeros([4,10])\n",
    "    fscore_array = np.zeros([4,10])\n",
    "    TN_array = np.zeros([4])\n",
    "    FP_array = np.zeros([4])\n",
    "    FN_array = np.zeros([4])\n",
    "    TP_array = np.zeros([4])   \n",
    "    parameters = {'n_neighbors':(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)}\n",
    "    KNN = KNeighborsClassifier()\n",
    "    clf = GridSearchCV(KNN, parameters)\n",
    "    for j in range(4):\n",
    "        Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y[:,2], test_size=0.25) # Modificar metodología de validación\n",
    "        scaler = MinMaxScaler() #Escala entre 0 y 1\n",
    "        Xtrain = scaler.fit_transform(Xtrain)\n",
    "        Xtest = scaler.transform(Xtest)\n",
    "\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        Yest = clf.best_estimator_.predict(Xtest)\n",
    "\n",
    "        #code for calculating recall \n",
    "        sensitivity = recall_score(Ytest, Yest, average=None)\n",
    "        sensitivity_array[j] = sensitivity\n",
    "\n",
    "        #code for calculating accuracy \n",
    "        efficiency = accuracy_score(Ytest, Yest, normalize=True)\n",
    "        efficiency_array[j] = efficiency\n",
    "\n",
    "        #code for calculating precision \n",
    "        precision = precision_score(Ytest, Yest, average=None)\n",
    "        precision_array[j] = precision\n",
    "\n",
    "        #code for calculating f1 score \n",
    "        fscore = f1_score(Ytest, Yest, average=None)\n",
    "        fscore_array[j] = fscore\n",
    "\n",
    "        #code for calculating confusion matrix \n",
    "        _confusion_matrix_ = confusion_matrix(Ytest, Yest)\n",
    "        TN_array[j] = _confusion_matrix_[0][0]\n",
    "        FP_array[j] = _confusion_matrix_[0][1]\n",
    "        FN_array[j] = _confusion_matrix_[1][0]\n",
    "        TP_array[j] = _confusion_matrix_[1][1]\n",
    "                \n",
    "        error[j] = classification_error(Yest, Ytest)\n",
    "          \n",
    "    if graficar == True:\n",
    "        j = 0\n",
    "        cm = confusion_matrix(Ytest, Yest, normalize='pred')\n",
    "        columnas = ['Clase %s'%(i) for i in list(ascii_uppercase)[0:len(np.unique(Yest))]]\n",
    "        df_cm = pd.DataFrame(cm,index = columnas, columns = columnas)\n",
    "        grafica = sns.heatmap(df_cm, cmap = 'Greens', annot = True)\n",
    "        grafica.set(xlabel = 'Verdaderos', ylabel = 'Predicciones')\n",
    "\n",
    "    return clf.best_estimator_, str(np.mean(sensitivity_array)), str(np.std(sensitivity_array)), str(np.mean(efficiency_array)), str(np.std(efficiency_array)), str(np.mean(precision_array)), str(np.std(precision_array)),  str(np.mean(fscore_array)), str(np.std(fscore_array)), str(np.mean(error)), str(np.std(error)), str(time.time()-initial_time)\n",
    "\n",
    "modelKNN, sensitivity, sensitivity_interval, efficiency, efficiency_interval, precision, precision_interval, f, f_interval, error, std_error, tiempo = KNN_model_CV(graficar = True)\n",
    "print('The best model was', modelKNN,\n",
    "        '\\n\\nSensibilidad:', sensitivity,'- Intervalo Sensibilidad:', sensitivity_interval,\n",
    "        '\\nEficiencia:', efficiency,'- Intervalo Eficiencia:', efficiency_interval,\n",
    "        '\\nPrecision:', precision,'- Intervalo Precision:', precision_interval,\n",
    "        '\\nF-Score:',f,'- Intervalo F-Score:', f_interval,\n",
    "        '\\nError_Prueba:',error,'- Intervalo Error:', std_error,\n",
    "        '\\nTiempo ejecución:', tiempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
